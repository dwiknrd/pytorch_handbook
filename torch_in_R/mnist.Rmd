---
title: 'Torch in R'
author: "Team Algoritma"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged 
    highlight: tango
    theme: cosmo
    toc: yes  
    toc_float:
      collapsed: no
    number_sections: true
---

```{r setup, include=FALSE}
# scientific notation
options(scipen = 99)

# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)
```

<style>
body {
text-align: justify}
</style>

# Background

Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. These neural networks attempt to simulate the behavior of the human brain allowing it to “learn” from large amounts of data. Not only possible to apply in large amounts of data, but it also allows to deal with unstructured data such as image, text, and sound.

You can try to implement a neural network from scratch. But, do you think this is a good idea when building deep learning models on a real-world dataset? It is definitely possible if you have days or weeks to spare waiting for the model to build. But, in any conditions we have many constrain to do it e.g time and cost. 

Here is the good news, now we can use deep learning frameworks that aim to simplify the implementation of complex deep learning mmodels. Using these frameworks, we can implement complex models like convolutional neural networks in no time.

A deep learning framework is a tool that allows us to build deep learning models more easily and quickly. They provide a clear and concise way for defining models using a collection of pre-built and optimized components. Instead of writing hundreds of lines of code, we can use a suitable framework to help us to build such a model quickly. 

## Keras VS PyTorch

![](assets/keras vs pytorch.png)

|Keras | PyTorch|
|---|---|
|Keras was released in March 2015| While PyTorch was released in October 2016|
|Keras has a high level API| While PyTorch has a low level API|
|Keras is comparatively slower in speed | While PyTorch has a higher speed than Keras, suitable for high performance|
|Keras has a simple architecture, making it more readable and easy to use | While PyTorch has very low readablility due to a complex architecture|
|Keras has a smaller community support | While PyTorch has a stronger community support|
|Keras is mostly used for small datasets due to its slow speed | While PyTorch is preferred for large datasets and high performance|
|Debugging in Keras is difficult due to presence of computational junk | While debugging in PyTorch is easier and faster|
|Keras provides static computation graphs | While PyTorch provides dynamic computation graphs|
|Backend for Keras include:TensorFlow, Theano and Microsoft CNTK backend | While PyTorch has no backend implementation|

# Installation

Please ensure you have installed `torch` package. You also can install the following package (`dplyr`, `ggplot2`, and `tidyr`) to run sample RMarkdown. Here is the short instruction on how to install `torch` package in R.

1. Please install `torch` packages in your console

   `install.packages("torch")`

2. If there is a problem related to the `Rcpp` package, you can install the `Rcpp` package in your console

   `install.packages("Rcpp")`

[For more details you can visit](https://torch.mlverse.org/docs/articles/installation.html)

# Load Data

## Working with Images

Let's start implement our existing knowledge of neural network using `torch` in **R** to solve an **image classification** problem. We'll use famous [**MNIST Handwritten Digits Data**](https://www.kaggle.com/c/digit-recognizer/data) as our training dataset. It consists of 28 by 28 pixels grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents. There are two files of data to be downloaded: `train.csv` is pixel data with actual digit label, whereas `test.csv` is pixel data **without** the actual digit label. Here are some sample image from dataset: ![mnist](https://i.imgur.com/CAYnuo1.jpeg)

It's evident that these images are relatively small in size, and recognizing the digits can sometimes be challenging even for the human eye. While it's useful to look at these images, there's just one problem here: Torch doesn't know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset.

Torch Dataset allow us to specify one or more transformation functions that are applied to the images as they are loaded. We'll use the `torch.tensor()` to convert the pixel values into tensors.


```{r}
library(torch)

MNISTDataset <- dataset(
  name = "MNISTDataset",
  
  # init function
  initialize = function(df) {
    target_col <- 'label'
    self$label_exist <- target_col %in% names(df)
    
    if (self$label_exist) {
      X <- df[,-which(names(df) == target_col)]
      y <- df[,target_col]
      
      # perlu tambahin satu soalnya di R ga bisa akses index 0
      self$y <- torch_tensor(as.integer(y+1))
    } else {
      X <- df
    }
    
    self$X <- torch_tensor(as.matrix(X), dtype=torch_float32())
    
    self$X <- self$X / 255
  },
  
  .getitem = function(index) {
    if (self$label_exist) {
      list(X = self$X[index,], y = self$y[index])
    } else {
      list(X = self$X[index,])
    }
  },
  
  .length = function() {
    self$X$size()[1]
  }
)
```

```{r}
mnist <- read.csv("mnist/train.csv")
```

```{r}
SPLIT_PROP <- 0.8
BATCH_SIZE <- 16
```

## Training and Validation Datasets

While building real-world machine learning models, it is quite common to split the dataset into three parts:

- **Training set** - used to train the model, i.e., compute the loss and adjust the model's weights using gradient descent.
- **Validation set** - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model.
- **Test set** - used to compare different models or approaches and report the model's final accuracy.

In the MNIST dataset, there are 42,000 images in `train.csv` and 784 images in `test.csv`. The test set is standardized so that different researchers can report their models' results against the same collection of images.

Since there's no predefined validation set, we must manually split the 42,000 images into training, validation, and test datasets. Let's set aside 42,000 randomly chosen images for validation.

```{r}
# ga dibuat jadi objek dataset dulu soalnya ga ada random_split(), adanya torch_split() untuk ngesplit tensor
spec <- c(train = SPLIT_PROP*SPLIT_PROP, val = SPLIT_PROP*(1-SPLIT_PROP), test = (1-SPLIT_PROP))

g <- sample(cut(
  seq(nrow(mnist)),
  nrow(mnist)*cumsum(c(0,spec)),
  labels = names(spec)
))

res <- split(mnist, g)

print(nrow(res$train))
print(nrow(res$val))
print(nrow(res$test))
```

```{r}
train_loader <- dataloader(
  MNISTDataset(res$train),
  batch_size = BATCH_SIZE,
  shuffle = TRUE)

val_loader <- dataloader(
  MNISTDataset(res$val),
  batch_size = BATCH_SIZE,
  shuffle = TRUE)

test_loader <- dataloader(
  MNISTDataset(res$test),
  batch_size = BATCH_SIZE,
  shuffle = TRUE)

print(length(train_loader))
print(length(val_loader))
print(length(test_loader))
```

It's essential to choose a random sample for creating a validation and test set. Training data is often sorted by the target labels, i.e., images of 0s, followed by 1s, followed by 2s, etc. If we create a validation set using the last 20% of images, it would only consist of 8s and 9s. In contrast, the training set would contain no 8s or 9s. Such a training-validation would make it impossible to train a useful model.

We can now create `DataLoader` to help us load the data in batches. We'll use a batch size of 16. We set **`shuffle = TRUE`** for the data loader to ensure that the batches generated in each epoch are different. This randomization helps generalize & speed up the training process.

# Visualize Data

Let's check the proportion of each class labels, since it is important to ensure that the model can learn each digit in a balanced and fair manner. We check the distribution of digits in training, validation, and also test set.

```{r}
library(dplyr)
library(ggplot2)
mnist %>% 
  select(label) %>% 
  mutate(sample = g) %>% 
  ggplot(aes(x=label, fill=sample)) +
  geom_bar() +
  facet_wrap(~sample, scales = "free_y") +
  theme_minimal()
```

```{r}
# viz image
vizMNIST <- function(input){
  
  dimmax <- sqrt(ncol(input[,-1]))
  
  dimn <- ceiling(sqrt(nrow(input)))
  par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
  
  for (i in 1:nrow(input)){
      m1 <- matrix(input[i,2:ncol(input)], nrow=dimmax, byrow=T)
      m1 <- apply(m1, 2, as.numeric)
      m1 <- t(apply(m1, 2, rev))
      
      image(1:dimmax, 1:dimmax, m1,
            col=grey.colors(255), xaxt = 'n', yaxt = 'n')
      text(2, 20, col="white", cex=1.2, input[i, 'label'])
  }
  
}
```

The distribution of our class labels do seem to be spread out quite evenly, so there's no problem. Next, we visualize a couple of images from `train`:

```{r}
mnist_train <- mnist %>% 
  mutate(sample = g) %>% 
  filter(sample == "train") %>% 
  select(-sample)

# visualize
vizMNIST(input = mnist_train[1:25,])
```

# Define Model Architecture

We'll create a neural network with three layers: two hidden and one output layer. Additionally, we'll use `nn_relu` activation function between each layer. Let's create a **`nn_sequential()`** object, which consists of linear fully-connected layer:

- The input size is 784 nodes as the MNIST dataset consists of 28 by 28 pixels
- The hidden sizes are 128 and 64 respectively, this number can be increased or decreased to change the learning capacity of the model
- The output size is 10 as the MNIST dataset has 10 target classes (0 to 9)

```{r}
# ALTERNATIVE 2: Sequential, Keras style

input_size <- 784
hidden_sizes <- c(128, 64)
output_size <- 10

model <- nn_sequential(
  # Layer 1
  nn_linear(input_size, hidden_sizes[1]),
  nn_relu(), 

  # Layer 2
  nn_linear(hidden_sizes[1], hidden_sizes[2]),
  nn_relu(),

  # Layer 3
  nn_linear(hidden_sizes[2], output_size)
)

model
```

A little bit about the mathematical detail: The image vector of size 784 are transformed into intermediate output vector of length 128 then 64 by performing a matrix multiplication of inputs matrix. Thus, input and layer1 outputs have linear relationship, i.e., each element of layer outputs is a weighted sum of elements from inputs. Thus, even as we train the model and modify the weights, layer1 can only capture linear relationships between inputs and outputs.

![layers](https://i.imgur.com/inXsLuq.png)


Activation function such as **Rectified Linear Unit (ReLU)** is used to introduce non-linearity to the model. It has the formula `relu(x) = max(0,x)` i.e. it simply replaces negative values in a given tensor with the value 0. We refer to ReLU as the activation function, because for each input certain outputs are activated (those with non-zero values) while others turned off (those with zero values). ReLU can be seen visually as follows:

![relu](https://i.imgur.com/yijV4xF.png)


The output layer returns a batch of vectors of size 10. This predicted output is then being compared with actual label, quantified by using **`nn_cross_entropy_loss()`**. It combines `LogSoftmax` and `NLLLoss` (Negative Log Likelihood Loss) in one single class.

The loss value is used to update the parameter weights in the model. The parameter update algorithm can be implemented via an optimizer. In this case, we are using **`optim_adam()`** with learning rate (`lr=0.001`).

```{r}
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
```

# Train the Model

Using `torch`, we have to manually loop the data per epoch to **train the model**. Here are the training loop for one epoch:

1. Clear gradients of all optimized variables from `optimizer`
2. Forward pass: compute predicted output by passing input data to the `model`
3. Calculate the loss based on specified `criterion`
4. Backward pass: compute gradient of the loss with respect to `model` parameters
5. Perform parameter update using `optimizer` algorithm
6. Accumulate training loss and accuracy

Inside the training loop, we also **validate the model** by performing the following steps:

1. Forward pass: compute predicted output by passing input data to the `model`
2. Calculate the loss based on specified `criterion`
3. Accumulate validation loss and accuracy

But first let's us define `evaluate_accuracy` to calculate accuracy given the predicted `logits` and `y_true` actual label.

```{r}
evaluate_accuracy <- function(logits, y_true) {
  y_pred <- logits$argmax(dim=2)
  
  correct_pred <- (y_pred == y_true)
  acc <- correct_pred$sum()$item() / y_true$size()
  
  acc * 100
}
```

Define training loop with the following parameters:

- `model`: untrained model
- `train_loader`: data train of a `dataloader` object
- `val_loader`: data validation of a `dataloader` object
- `criterion`: loss function to be optimized
- `optimizer`: optimization algorithm to used on the `model` parameters
- `n_epochs`: number of training epochs
- `model_file_name`: file name of serialized model. During the training loop, model with the lowest validation loss will be saved as a serialized model with `.rt` extension.

```{r}
train <- function(model, train_loader, val_loader, criterion, optimizer, n_epochs, model_file_name='model.rt'){
  # initialize container variable for model performance results per epoch
  history <- list(
    n_epochs = 1:n_epochs,
    loss = list(train = c(), val = c()),
    acc = list(train = c(), val = c())
  )
  
  # initialize tracker for minimum validation loss
  val_loss_min <- Inf
  
  # loop per epoch
  for (epoch in 1:n_epochs) {
    # initialize container for training performance
    train_acc <- c()
    train_losses <- c()
    
    ###################
    # train the model #
    ###################
    
    # prepare model for training
    model$train()
    
    # loop for each batch
    coro::loop(for (b in train_loader) {
      # STEP 1: clear gradients
      optimizer$zero_grad()
      # STEP 2: forward pass
      output <- model(b$X)
      # STEP 3: calculate the loss
      loss <- criterion(output, b$y)
      # STEP 4: backward pass
      loss$backward()
      # STEP 5: perform parameter update
      optimizer$step()
      # STEP 6: accumulate training loss and accuracy
      train_losses <- c(train_losses, loss$item())
      acc <- evaluate_accuracy(output, b$y)
      train_acc <- c(train_acc, acc)
    })
  
    ######################
    # validate the model #
    ######################
    
    # initialize container for validation performance
    val_acc <- c()
    val_losses <- c()
    
    # prepare model for evaluation
    model$eval()
    
    # loop for each batch
    coro::loop(for (b in val_loader) {
      # STEP 1: forward pass
      output <- model(b$X)
      # STEP 2: calculate the loss
      loss <- criterion(output, b$y)
      # STEP 3: accumulate validation loss and accuracy
      val_losses <- c(val_losses, loss$item())
      acc <- evaluate_accuracy(output, b$y)
      val_acc <- c(val_acc, acc)
    })
    
    ####################   
    # model evaluation #
    ####################
    
    # calculate average loss over an epoch
    avg_train_loss <- mean(train_losses)
    avg_val_loss <- mean(val_losses)
    history[['loss']][['train']] <- c(history[['loss']][['train']], avg_train_loss)
    history[['loss']][['val']] <- c(history[['loss']][['val']], avg_val_loss)
    
    # calculate average accuracy over an epoch
    avg_train_acc <- mean(train_acc)
    avg_val_acc <- mean(val_acc)
    history[['acc']][['train']] <- c(history[['acc']][['train']], avg_train_acc)
    history[['acc']][['val']] <- c(history[['acc']][['val']], avg_val_acc)
    
    # print training progress per epoch
    cat(sprintf("Epoch %d | Train Loss: %3.3f | Val Loss: %3.3f | Train Acc: %.2f | Val Acc: %.2f\n",
                epoch, avg_train_loss, avg_val_loss, avg_train_acc, avg_val_acc))
    
    # save model if validation loss has decreased
    if (avg_val_loss <= val_loss_min) {
      cat(sprintf("Validation loss decreased (%.5f --> %.5f)  Saving model to %s...\n",
                  val_loss_min, avg_val_loss, model_file_name))
      torch_save(model, model_file_name)
      val_loss_min <- avg_val_loss
    }
  }
  
  # return model performance history
  return(history)
}
```

```{r eval=FALSE, include=FALSE}
history <- train(
  model, train_loader, val_loader,
  criterion, optimizer,
  n_epochs = 20)
```

```{r eval=FALSE, include=FALSE}
# save history list as RDS file
saveRDS(history, "history.rds")
```

Visualize the loss and accuracy from `history` to see the model performance for each epoch.

```{r}
# load previously saved history list
history <- readRDS("history.rds")

library(tidyr)

# visualization
data.frame(history) %>% 
  pivot_longer(cols = -n_epochs) %>% 
  separate(name, into = c("metric", "data"), sep = "\\.") %>% 
  ggplot(aes(x = n_epochs, y = value, color = data)) +
  geom_line() +
  facet_wrap(~metric, scales = "free_y")
```

From the visualization, we can deduct that the model is a good enough since the difference of model performance on training and validation is not too much different, and the accuracy is converging at 96-97%.

# Test the Model

In this section, we are going to test the model performance by using confusion matrix. Here are the steps:

1. Forward pass: compute predicted output by passing input data to the **trained** `model`
2. Get predicted label by retrieve index with the largest logit value per observation
3. Append actual and predicted label to `y_test` and `y_pred` respectively

Special notes: We must subtract the label with one to convert from index to class label, since R is a programming language with one-based indexing.

```{r}
# load the best model
model <- torch_load("model.rt")
```

```{r}
# initialize container
y_true <- c()
y_pred <- c()

# prepare model for evaluation
model$eval()

# loop for each batch
coro::loop(for (b in test_loader) {
  # STEP 1: forward pass
  output <- model(b$X)
  # STEP 2: get predicted label
  labels <- b$y - 1 # -1 = convert from index to class label
  
  # torch_max returns a list
  # position 1 containing the values 
  # position 2 containing the respective indices
  predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # -1 = convert from index to class label
  
  # STEP 3: append actual and predicted label
  for (i in 1:BATCH_SIZE){
    y_true <- c(y_true, labels[i]$item())
    y_pred <- c(y_pred, predicted[i]$item())
  }
})

```

Create a confusion matrix heatmap using `yardstick` and classification report by using `caret` to know the final model performance on test data.

```{r}
library(yardstick)

data.frame(y_pred = factor(y_pred),
           y_true = factor(y_true)) %>%
  conf_mat(y_pred, y_true) %>% 
  autoplot(type = "heatmap") + 
  scale_fill_distiller(palette = 7, direction = "reverse")
```

```{r}
library(caret)
confusionMatrix(as.factor(y_pred), as.factor(y_true))
```

> Eventually, the model achieves 96.51% accuracy and other metrics reach >= 90% on unseen data.

# Predict the Unlabeled Data

In this last section, we use the trained model to predict the unlabeled data from `test.csv`, which only consists 784 columns of pixel without the actual label.

```{r}
mnist_data_unlabeled <- read.csv("mnist/test.csv")

unlabeled_loader <- dataloader(
  MNISTDataset(mnist_data_unlabeled),
  batch_size = 1)

print(length(unlabeled_loader))
```

Each image is feeded into the model with `batch_size = 1` and get the predicted labels for the first 20 images.

```{r}
# initialize container for predicted labels
y_pred_unlabeled <- c()

# loop for each data
for (i in 1:20) {
  # forward pass
  output <- model(unlabeled_loader$dataset$X[i,])
  
  # torch_max returns a list
  # position 1 containing the values 
  # position 2 containing the respective indices
  predicted <- torch_max(output$data(), dim = 1)[[2]] - 1 # -1 = convert from index to class label
  
  # prob <- nnf_softmax(output$data(), dim = 1)
  
  # append predicted label
  y_pred_unlabeled <- c(y_pred_unlabeled, predicted$item())
}
```


The visualization below tells us the predicted label given an image.

```{r}
mnist_data_unlabeled[1:length(y_pred_unlabeled),] %>% 
  mutate(label = y_pred_unlabeled) %>% 
  vizMNIST()
```

As we can see, majority of the images are classified correctly except for certain cases, such as:

- First row fourth column = actual digit is 0 but classified as 9
- Second row first column = actual digit is 7 but classified as 9 

# References

- [Wikipedia](https://en.wikipedia.org/wiki/Deep_learning)
- [Deep Learning](https://www.ibm.com/cloud/learn/deep-learning)
- [Keras vs PyTorch](https://www.geeksforgeeks.org/keras-vs-pytorch/)
- [Torch for R](https://medium.com/pytorch/please-allow-me-to-introduce-myself-torch-for-r-9ea0f361ea7e)
- [Function `Dataset()`](https://torch.mlverse.org/docs/articles/examples/dataset.html)
- [Custom Dataset](https://torch.mlverse.org/start/custom_dataset/)

# Further Reading

There's a lot of scope to experiment here, here are a few ideas:

* Try changing the size of the hidden layer, or add more hidden layers and see if you can achieve a higher accuracy.

* Try changing the batch size and learning rate to see if you can achieve the same accuracy in fewer epochs.

* Compare the training times on a CPU vs. GPU. Do you see a significant difference. How does it vary with the size of the dataset and the size of the model (no. of weights and parameters)?

* Try building a model for a different dataset, such as the [CIFAR10 or CIFAR100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html).


Here are some references for further reading:

* [A visual proof that neural networks can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html), also known as the Universal Approximation Theorem.

* [But what *is* a neural network?](https://www.youtube.com/watch?v=aircAruvnKk) - A visual and intuitive introduction to what neural networks are and what the intermediate layers represent

* [Stanford CS229 Lecture notes on Backpropagation](http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf) - for a more mathematical treatment of how gradients are calculated and weights are updated for neural networks with multiple layers.