---
title: 'Torch in R'
author: "Team Algoritma"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged 
    highlight: tango
    theme: cosmo
    toc: yes  
    toc_float:
      collapsed: no
    number_sections: true
---

```{r setup, include=FALSE}
# scientific notation
options(scipen = 99)

# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)
```

<style>
body {
text-align: justify}
</style>

# Background

Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. These neural networks attempt to simulate the behavior of the human brain allowing it to “learn” from large amounts of data. Not only possible to apply in large amounts of data, but it also allows to deal with unstructured data such as image, text, and sound.

You can try to implement a neural network from scratch. But, do you think this is a good idea when building deep learning models on a real-world dataset? It is definitely possible if you have days or weeks to spare waiting for the model to build. But, in any conditions we have many constrain to do it e.g time and cost. 

Here is the good news, now we can use deep learning frameworks that aim to simplify the implementation of complex deep learning mmodels. Using these frameworks, we can implement complex models like convolutional neural networks in no time.

A deep learning framework is a tool that allows us to build deep learning models more easily and quickly. They provide a clear and concise way for defining models using a collection of pre-built and optimized components. Instead of writing hundreds of lines of code, we can use a suitable framework to help us to build such a model quickly. 

## Keras VS PyTorch

![](assets/keras vs pytorch.png)

|Keras | PyTorch|
|---|---|
|Keras was released in March 2015| While PyTorch was released in October 2016|
|Keras has a high level API| While PyTorch has a low level API|
|Keras is comparatively slower in speed | While PyTorch has a higher speed than Keras, suitable for high performance|
|Keras has a simple architecture,making it more readable and easy to use | While PyTorch has very low readablility due to a complex architecture|
|Keras has a smaller community support | While PyTorch has a stronger community support|
|Keras is mostly used for small datasets due to its slow speed | While PyTorch is preferred for large datasets and high performance|
|Debugging in Keras is difficult due to presence of computational junk | While debugging in PyTorch is easier and faster|
|Keras provides static computation graphs | While PyTorch provides dynamic computation graphs|
|Backend for Keras include:TensorFlow, Theano and Microsoft CNTK backend | While PyTorch has no backend implementation|

# Data

Let us start our neural network experience using `torch` in R by first preparing the dataset. You will use `Digit Recognizer (MNIST)` dataset which can be downloaded on [the following page](https://www.kaggle.com/c/digit-recognizer/data). Data to download are `train.csv` as train data and `test.csv` as test data. Both of the data store digit images measuring 28 x 28 pixels for 10 different categories.

# Installation

Please ensure you have installed `torch` package. You also can install the following package (`dplyr`, `ggplot2`, and `tidyr`) to run sample RMarkdown. Here is the short instruction on how to install `torch` package in R.

1. Please install `torch` packages in your console

   `install.packages("torch")`

2. If there is a problem related to the `Rcpp` package, you can install the `Rcpp` package in your console

   `install.packages("Rcpp")`

[For more details you can visit](https://torch.mlverse.org/docs/articles/installation.html)

# Load Data

```{r}
# install.packages("torch")
library(torch)
# error Rcpp: remove.packages("Rcpp") -> install.packages("Rcpp")

MNISTDataset <- dataset(
  name = "MNISTDataset",
  
  # init function
  initialize = function(df) {
    target_col <- 'label'
    self$label_exist <- target_col %in% names(df)
    
    if (self$label_exist) {
      X <- df[,-which(names(df) == target_col)]
      y <- df[,target_col]
      
      # perlu tambahin satu soalnya di R ga bisa akses index 0
      self$y <- torch_tensor(as.integer(y+1))
    } else {
      X <- df
    }
    
    self$X <- torch_tensor(as.matrix(X), dtype=torch_float32())
    
    self$X <- self$X / 255
  },
  
  .getitem = function(index) {
    if (self$label_exist) {
      list(X = self$X[index,], y = self$y[index])
    } else {
      list(X = self$X[index,])
    }
  },
  
  .length = function() {
    self$X$size()[1]
  }
)
```

```{r}
mnist <- read.csv("mnist/train.csv")
```

```{r}
SPLIT_PROP <- 0.8
BATCH_SIZE <- 16
```

```{r}
# ga dibuat jadi objek dataset dulu soalnya ga ada random_split(), adanya torch_split() untuk ngesplit tensor
spec <- c(train = SPLIT_PROP*SPLIT_PROP, val = SPLIT_PROP*(1-SPLIT_PROP), test = (1-SPLIT_PROP))

g <- sample(cut(
  seq(nrow(mnist)),
  nrow(mnist)*cumsum(c(0,spec)),
  labels = names(spec)
))

res <- split(mnist, g)

print(nrow(res$train))
print(nrow(res$val))
print(nrow(res$test))
```

```{r}
train_loader <- dataloader(
  MNISTDataset(res$train),
  batch_size = BATCH_SIZE,
  shuffle = TRUE)

val_loader <- dataloader(
  MNISTDataset(res$val),
  batch_size = BATCH_SIZE,
  shuffle = TRUE)

test_loader <- dataloader(
  MNISTDataset(res$test),
  batch_size = BATCH_SIZE,
  shuffle = TRUE)

print(length(train_loader))
print(length(val_loader))
print(length(test_loader))
```

# Visualize Data

```{r}
library(dplyr)
library(ggplot2)
mnist %>% 
  select(label) %>% 
  mutate(sample = g) %>% 
  ggplot(aes(x=label, fill=sample)) +
  geom_bar() +
  facet_wrap(~sample, scales = "free_y") +
  theme_minimal()
```

```{r}
# viz image
vizTrain <- function(input){
  
  dimmax <- sqrt(ncol(mnist_train[,-1]))
  
  dimn <- ceiling(sqrt(nrow(input)))
  par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
  
  for (i in 1:nrow(input)){
      m1 <- matrix(input[i,2:ncol(input)], nrow=dimmax, byrow=T)
      m1 <- apply(m1, 2, as.numeric)
      m1 <- t(apply(m1, 2, rev))
      
      image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
      text(2, 20, col="white", cex=1.2, mnist_train[i, 1])
  }
  
}
```

```{r}
mnist_train <- mnist %>% 
  mutate(sample = g) %>% 
  filter(sample == "train") %>% 
  select(-sample)

# visualize
vizTrain(input = mnist_train[1:25,])
```

# Define Model Architecture

```{r}
# ALTERNATIVE 2: Sequential, Keras style

input_size <- 784
hidden_sizes <- c(128, 64)
output_size <- 10

model <- nn_sequential(
  # Layer 1
  nn_linear(input_size, hidden_sizes[1]),
  nn_relu(), 

  # Layer 2
  nn_linear(hidden_sizes[1], hidden_sizes[2]),
  nn_relu(),

  # Layer 3
  nn_linear(hidden_sizes[2], output_size)
)

model
```

```{r}
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
```

```{r}
evaluate_accuracy <- function(logits, y_true) {
  y_pred <- logits$argmax(dim=2)
  
  correct_pred <- (y_pred == y_true)
  acc <- correct_pred$sum()$item() / y_true$size()
  
  acc * 100
}
```


```{r}
# train + val loop: https://torch.mlverse.org/start/custom_dataset/
# accuracy + save model: https://anderfernandez.com/en/blog/how-to-create-neural-networks-with-torch-in-r/

n_epochs <- 2

history <- list(
  n_epochs = 1:n_epochs,
  loss = list(train = c(),
              val = c()),
  acc = list(train = c(),
              val = c())
  )

val_loss_min <- Inf

for (epoch in 1:n_epochs) {
  
  # TRAINING DATA
  model$train()
  train_losses <- c()
  train_acc <- c()
  coro::loop(for (b in train_loader) {
    
    optimizer$zero_grad()
    output <- model(b$X)
    loss <- criterion(output, b$y)
    loss$backward()
    optimizer$step()
    train_losses <- c(train_losses, loss$item())
    
    acc <- evaluate_accuracy(output, b$y)
    train_acc <- c(train_acc, acc)
  
  })

  # VALIDATION DATA
  model$eval()
  val_losses <- c()
  val_acc <- c()
  coro::loop(for (b in val_loader) {
    
    output <- model(b$X)
    loss <- criterion(output, b$y)
    val_losses <- c(val_losses, loss$item())
    
    acc <- evaluate_accuracy(output, b$y)
    val_acc <- c(val_acc, acc)
    
  })
  
  # EVAL
  avg_train_loss <- mean(train_losses)
  avg_val_loss <- mean(val_losses)
  avg_train_acc <- mean(train_acc)
  avg_val_acc <- mean(val_acc)
  
  history[['loss']][['train']] <- c(history[['loss']][['train']], avg_train_loss)
  history[['loss']][['val']] <- c(history[['loss']][['val']], avg_val_loss)
  history[['acc']][['train']] <- c(history[['acc']][['train']], avg_train_acc)
  history[['acc']][['val']] <- c(history[['acc']][['val']], avg_val_acc)

  cat(sprintf("Epoch %d | Train Loss: %3.3f | Val Loss: %3.3f | Train Acc: %.2f | Val Acc: %.2f\n",
              epoch, avg_train_loss, avg_val_loss, avg_train_acc, avg_val_acc))
  
  if (avg_val_loss <= val_loss_min) {
    model_file_name <- "model.rt"

    cat(sprintf("Validation loss decreased (%.5f --> %.5f)  Saving model to %s...\n",
                val_loss_min, avg_val_loss, model_file_name))
    torch_save(model, model_file_name)
    val_loss_min <- avg_val_loss
  }
}
```

```{r}
torch_save(model, "model.rt")
saveRDS(history, "history.rds")
```

```{r}
# history <- readRDS("history.rds")
library(tidyr)
data.frame(history) %>% 
  pivot_longer(cols = -n_epochs) %>% 
  separate(name, into = c("metric", "data"), sep = "\\.") %>% 
  ggplot(aes(x = n_epochs, y = value, color = data)) +
  geom_line() +
  facet_wrap(~metric, scales = "free_y")
```

# Test the Model

```{r}
model <- torch_load("model.rt")
# model <- torch_load("model_softmax.rt")

test_losses <- c()
total <- 0
correct <- 0

for (b in enumerate(test_loader)) {
  output <- model(b[[1]]$to())
  labels <- b[[2]]$to()
  loss <- nnf_cross_entropy(output, labels)
  test_losses <- c(test_losses, loss$item())
  # torch_max returns a list, with position 1 containing the values 
  # and position 2 containing the respective indices
  predicted <- torch_max(output$data(), dim = 2)[[2]]
  total <- total + labels$size(1)
  # add number of correct classifications in this batch to the aggregate
  correct <- correct + (predicted == labels)$sum()$item()
}

mean(test_losses)
```

```{r}
test_accuracy <-  correct/total
test_accuracy
```

# Predict the Unlabeled Data

```{r}
mnist_data_unlabeled <- read.csv("mnist/test.csv")
```

```{r}
unlabeled_loader <- dataloader(
  MNISTDataset(mnist_data_unlabeled),
  batch_size = BATCH_SIZE,
  shuffle = TRUE)

print(length(unlabeled_loader))
```

```{r}
# viz image
vizUnlabled <- function(input){
  
  dimmax <- sqrt(ncol(mnist_data_unlabeled))
  
  dimn <- ceiling(sqrt(nrow(input)))
  
  par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
  
  for (i in 1:nrow(input)){
      m1 <- matrix(input[i,1:ncol(input)], nrow=dimmax, byrow=T)
      m1 <- apply(m1, 2, as.numeric)
      m1 <- t(apply(m1, 2, rev))
      
      image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
  }
  
}
```

```{r}
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:25,])
```

```{r}
predicted <- assign("list", NULL, envir = .GlobalEnv)

for (b in enumerate(unlabeled_loader)) {
  output <- model(b[[1]]$to())
  predicted <- torch_max(output$data(), dim = 2)[[2]]
}
```

```{r}
predicted %>% as_array()
```

# References:

- dataset func: https://torch.mlverse.org/docs/articles/examples/dataset.html
- split: https://stackoverflow.com/questions/36068963/r-how-to-split-a-data-frame-into-training-validation-and-test-sets
- [Wikipedia](https://en.wikipedia.org/wiki/Deep_learning)
- [Deep Learning](https://www.ibm.com/cloud/learn/deep-learning)
- [Keras vs PyTorch](https://www.geeksforgeeks.org/keras-vs-pytorch/)
- [Torch for R](https://medium.com/pytorch/please-allow-me-to-introduce-myself-torch-for-r-9ea0f361ea7e)

