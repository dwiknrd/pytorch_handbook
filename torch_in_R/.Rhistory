# visualize
vizUnlabled(input = mnist_data_unlabeled[1:10,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:25,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:8,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:10,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:12,])
y_pred_unlabeled
y_pred_unlabeled
as.data.frame(y_pred_unlabeled = y_pred_unlabeled)
y_pred_unlabeled <- c()
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred, predicted[i]$item())
}
}
as.data.frame(y_pred_unlabeled = y_pred_unlabeled)
y_pred_unlabeled
data.frame(y_pred_unlabeled = y_pred_unlabeled)
data.frame(y_pred_unlabeled = y_pred_unlabeled) %>%
head(12)
mnist_data_unlabeled <- read.csv("mnist/test.csv")
unlabeled_loader <- dataloader(
MNISTDataset(mnist_data_unlabeled),
batch_size = BATCH_SIZE,
shuffle = TRUE)
print(length(unlabeled_loader))
# viz image
vizUnlabled <- function(input){
dimmax <- sqrt(ncol(mnist_data_unlabeled))
dimn <- ceiling(sqrt(nrow(input)))
par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
for (i in 1:nrow(input)){
m1 <- matrix(input[i,1:ncol(input)], nrow=dimmax, byrow=T)
m1 <- apply(m1, 2, as.numeric)
m1 <- t(apply(m1, 2, rev))
image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
}
}
y_pred_unlabeled <- c()
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred, predicted[i]$item())
}
}
# scientific notation
options(scipen = 99)
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.align = "center",
comment = "#>"
)
# install.packages("torch")
library(torch)
MNISTDataset <- dataset(
name = "MNISTDataset",
# init function
initialize = function(df) {
target_col <- 'label'
self$label_exist <- target_col %in% names(df)
if (self$label_exist) {
X <- df[,-which(names(df) == target_col)]
y <- df[,target_col]
# perlu tambahin satu soalnya di R ga bisa akses index 0
self$y <- torch_tensor(as.integer(y+1))
} else {
X <- df
}
self$X <- torch_tensor(as.matrix(X), dtype=torch_float32())
self$X <- self$X / 255
},
.getitem = function(index) {
if (self$label_exist) {
list(X = self$X[index,], y = self$y[index])
} else {
list(X = self$X[index,])
}
},
.length = function() {
self$X$size()[1]
}
)
SPLIT_PROP <- 0.8
BATCH_SIZE <- 16
mnist_data_unlabeled <- read.csv("mnist/test.csv")
unlabeled_loader <- dataloader(
MNISTDataset(mnist_data_unlabeled),
batch_size = BATCH_SIZE,
shuffle = TRUE)
print(length(unlabeled_loader))
# viz image
vizUnlabled <- function(input){
dimmax <- sqrt(ncol(mnist_data_unlabeled))
dimn <- ceiling(sqrt(nrow(input)))
par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
for (i in 1:nrow(input)){
m1 <- matrix(input[i,1:ncol(input)], nrow=dimmax, byrow=T)
m1 <- apply(m1, 2, as.numeric)
m1 <- t(apply(m1, 2, rev))
image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
}
}
model <- torch_load("model.rt")
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred, predicted[i]$item())
}
}
y_pred_unlabeled <- c()
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred_unlabeled, predicted[i]$item())
}
}
gc()
# scientific notation
options(scipen = 99)
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.align = "center",
comment = "#>"
)
library(torch)
MNISTDataset <- dataset(
name = "MNISTDataset",
# init function
initialize = function(df) {
target_col <- 'label'
self$label_exist <- target_col %in% names(df)
if (self$label_exist) {
X <- df[,-which(names(df) == target_col)]
y <- df[,target_col]
# perlu tambahin satu soalnya di R ga bisa akses index 0
self$y <- torch_tensor(as.integer(y+1))
} else {
X <- df
}
self$X <- torch_tensor(as.matrix(X), dtype=torch_float32())
self$X <- self$X / 255
},
.getitem = function(index) {
if (self$label_exist) {
list(X = self$X[index,], y = self$y[index])
} else {
list(X = self$X[index,])
}
},
.length = function() {
self$X$size()[1]
}
)
# scientific notation
options(scipen = 99)
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.align = "center",
comment = "#>"
)
library(torch)
MNISTDataset <- dataset(
name = "MNISTDataset",
# init function
initialize = function(df) {
target_col <- 'label'
self$label_exist <- target_col %in% names(df)
if (self$label_exist) {
X <- df[,-which(names(df) == target_col)]
y <- df[,target_col]
# perlu tambahin satu soalnya di R ga bisa akses index 0
self$y <- torch_tensor(as.integer(y+1))
} else {
X <- df
}
self$X <- torch_tensor(as.matrix(X), dtype=torch_float32())
self$X <- self$X / 255
},
.getitem = function(index) {
if (self$label_exist) {
list(X = self$X[index,], y = self$y[index])
} else {
list(X = self$X[index,])
}
},
.length = function() {
self$X$size()[1]
}
)
mnist <- read.csv("mnist/train.csv")
SPLIT_PROP <- 0.8
BATCH_SIZE <- 16
# ga dibuat jadi objek dataset dulu soalnya ga ada random_split(), adanya torch_split() untuk ngesplit tensor
spec <- c(train = SPLIT_PROP*SPLIT_PROP, val = SPLIT_PROP*(1-SPLIT_PROP), test = (1-SPLIT_PROP))
g <- sample(cut(
seq(nrow(mnist)),
nrow(mnist)*cumsum(c(0,spec)),
labels = names(spec)
))
res <- split(mnist, g)
print(nrow(res$train))
print(nrow(res$val))
print(nrow(res$test))
train_loader <- dataloader(
MNISTDataset(res$train),
batch_size = BATCH_SIZE,
shuffle = TRUE)
val_loader <- dataloader(
MNISTDataset(res$val),
batch_size = BATCH_SIZE,
shuffle = TRUE)
test_loader <- dataloader(
MNISTDataset(res$test),
batch_size = BATCH_SIZE,
shuffle = TRUE)
print(length(train_loader))
print(length(val_loader))
print(length(test_loader))
library(dplyr)
library(ggplot2)
mnist %>%
select(label) %>%
mutate(sample = g) %>%
ggplot(aes(x=label, fill=sample)) +
geom_bar() +
facet_wrap(~sample, scales = "free_y") +
theme_minimal()
# viz image
vizMNIST <- function(input){
dimmax <- sqrt(ncol(input[,-1]))
dimn <- ceiling(sqrt(nrow(input)))
par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
for (i in 1:nrow(input)){
m1 <- matrix(input[i,2:ncol(input)], nrow=dimmax, byrow=T)
m1 <- apply(m1, 2, as.numeric)
m1 <- t(apply(m1, 2, rev))
image(1:dimmax, 1:dimmax, m1,
col=grey.colors(255), xaxt = 'n', yaxt = 'n')
text(2, 20, col="white", cex=1.2, input[i, 'label'])
}
}
mnist_train <- mnist %>%
mutate(sample = g) %>%
filter(sample == "train") %>%
select(-sample)
# visualize
vizMNIST(input = mnist_train[1:25,])
# ALTERNATIVE 2: Sequential, Keras style
input_size <- 784
hidden_sizes <- c(128, 64)
output_size <- 10
model <- nn_sequential(
# Layer 1
nn_linear(input_size, hidden_sizes[1]),
nn_relu(),
# Layer 2
nn_linear(hidden_sizes[1], hidden_sizes[2]),
nn_relu(),
# Layer 3
nn_linear(hidden_sizes[2], output_size)
)
model
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
evaluate_accuracy <- function(logits, y_true) {
y_pred <- logits$argmax(dim=2)
correct_pred <- (y_pred == y_true)
acc <- correct_pred$sum()$item() / y_true$size()
acc * 100
}
train <- function(model, train_loader, val_loader, criterion, optimizer, n_epochs, model_file_name='model.rt'){
# initialize container variable for model performance results per epoch
history <- list(
n_epochs = 1:n_epochs,
loss = list(train = c(), val = c()),
acc = list(train = c(), val = c())
)
# initialize tracker for minimum validation loss
val_loss_min <- Inf
# loop per epoch
for (epoch in 1:n_epochs) {
# initialize container for training performance
train_acc <- c()
train_losses <- c()
###################
# train the model #
###################
# prepare model for training
model$train()
# loop for each batch
coro::loop(for (b in train_loader) {
# STEP 1: clear gradients
optimizer$zero_grad()
# STEP 2: forward pass
output <- model(b$X)
# STEP 3: calculate the loss
loss <- criterion(output, b$y)
# STEP 4: backward pass
loss$backward()
# STEP 5: perform parameter update
optimizer$step()
# STEP 6: accumulate training loss and accuracy
train_losses <- c(train_losses, loss$item())
acc <- evaluate_accuracy(output, b$y)
train_acc <- c(train_acc, acc)
})
######################
# validate the model #
######################
# initialize container for validation performance
val_acc <- c()
val_losses <- c()
# prepare model for evaluation
model$eval()
# loop for each batch
coro::loop(for (b in val_loader) {
# STEP 1: forward pass
output <- model(b$X)
# STEP 2: calculate the loss
loss <- criterion(output, b$y)
# STEP 3: accumulate validation loss and accuracy
val_losses <- c(val_losses, loss$item())
acc <- evaluate_accuracy(output, b$y)
val_acc <- c(val_acc, acc)
})
####################
# model evaluation #
####################
# calculate average loss over an epoch
avg_train_loss <- mean(train_losses)
avg_val_loss <- mean(val_losses)
history[['loss']][['train']] <- c(history[['loss']][['train']], avg_train_loss)
history[['loss']][['val']] <- c(history[['loss']][['val']], avg_val_loss)
# calculate average accuracy over an epoch
avg_train_acc <- mean(train_acc)
avg_val_acc <- mean(val_acc)
history[['acc']][['train']] <- c(history[['acc']][['train']], avg_train_acc)
history[['acc']][['val']] <- c(history[['acc']][['val']], avg_val_acc)
# print training progress per epoch
cat(sprintf("Epoch %d | Train Loss: %3.3f | Val Loss: %3.3f | Train Acc: %.2f | Val Acc: %.2f\n",
epoch, avg_train_loss, avg_val_loss, avg_train_acc, avg_val_acc))
# save model if validation loss has decreased
if (avg_val_loss <= val_loss_min) {
cat(sprintf("Validation loss decreased (%.5f --> %.5f)  Saving model to %s...\n",
val_loss_min, avg_val_loss, model_file_name))
torch_save(model, model_file_name)
val_loss_min <- avg_val_loss
}
}
# return model performance history
return(history)
}
history <- train(
model, train_loader, val_loader,
criterion, optimizer,
n_epochs = 20)
# save history list as RDS file
saveRDS(history, "history.rds")
# load previously saved history list
history <- readRDS("history.rds")
library(tidyr)
# visualization
data.frame(history) %>%
pivot_longer(cols = -n_epochs) %>%
separate(name, into = c("metric", "data"), sep = "\\.") %>%
ggplot(aes(x = n_epochs, y = value, color = data)) +
geom_line() +
facet_wrap(~metric, scales = "free_y")
# load the best model
model <- torch_load("model.rt")
# initialize container
y_true <- c()
y_pred <- c()
# prepare model for evaluation
model$eval()
# loop for each batch
coro::loop(for (b in test_loader) {
# STEP 1: forward pass
output <- model(b$X)
# STEP 2: get predicted label
labels <- b$y - 1 # -1 = convert from index to class label
# torch_max returns a list
# position 1 containing the values
# position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # -1 = convert from index to class label
# STEP 3: append actual and predicted label
for (i in 1:BATCH_SIZE){
y_true <- c(y_true, labels[i]$item())
y_pred <- c(y_pred, predicted[i]$item())
}
})
library(yardstick)
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 7, direction = "reverse")
library(caret)
confusionMatrix(as.factor(y_pred), as.factor(y_true))
mnist_data_unlabeled <- read.csv("mnist/test.csv")
unlabeled_loader <- dataloader(
MNISTDataset(mnist_data_unlabeled),
batch_size = 1)
print(length(unlabeled_loader))
# initialize container for predicted labels
y_pred_unlabeled <- c()
# loop for each data
for (i in 1:20) {
# forward pass
output <- model(unlabeled_loader$dataset$X[i,])
# torch_max returns a list
# position 1 containing the values
# position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # -1 = convert from index to class label
# prob <- nnf_softmax(output$data(), dim = 1)
# append predicted label
y_pred_unlabeled <- c(y_pred_unlabeled, predicted$item())
}
# initialize container for predicted labels
y_pred_unlabeled <- c()
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred_unlabeled, predicted[i]$item())
}
}
remove.packages("Rcpp")
install.packages("Rcpp")
install.packages("Rcpp")
# initialize container for predicted labels
y_pred_unlabeled <- c()
# loop for each data
for (i in 1:20) {
# forward pass
output <- model(unlabeled_loader$dataset$X[i,])
# torch_max returns a list
# position 1 containing the values
# position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # -1 = convert from index to class label
# prob <- nnf_softmax(output$data(), dim = 1)
# append predicted label
y_pred_unlabeled <- c(y_pred_unlabeled, predicted$item())
}
y_pred_unlabeled <- c()
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred_unlabeled, predicted[i]$item())
}
}
y_pred_unlabeled <- c()
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:16){
y_pred_unlabeled <- c(y_pred_unlabeled, predicted[i]$item())
}
}
# initialize container for predicted labels
y_pred_unlabeled <- c()
# loop for each data
for (i in 1:20) {
# forward pass
output <- model(unlabeled_loader$dataset$X[i,])
# torch_max returns a list
# position 1 containing the values
# position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 1)[[2]] - 1 # -1 = convert from index to class label
# prob <- nnf_softmax(output$data(), dim = 1)
# append predicted label
y_pred_unlabeled <- c(y_pred_unlabeled, predicted$item())
}
mnist_data_unlabeled[1:length(y_pred_unlabeled),] %>%
mutate(label = y_pred_unlabeled) %>%
vizMNIST()
