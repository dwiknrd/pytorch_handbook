} else {
X <- df
}
self$X <- torch_tensor(as.matrix(X), dtype=torch_float32())
self$X <- self$X / 255
},
.getitem = function(index) {
if (self$label_exist) {
list(X = self$X[index,], y = self$y[index])
} else {
list(X = self$X[index,])
}
},
.length = function() {
self$X$size()[1]
}
)
mnist <- read.csv("mnist/train.csv")
SPLIT_PROP <- 0.8
BATCH_SIZE <- 16
# ga dibuat jadi objek dataset dulu soalnya ga ada random_split(), adanya torch_split() untuk ngesplit tensor
spec <- c(train = SPLIT_PROP*SPLIT_PROP, val = SPLIT_PROP*(1-SPLIT_PROP), test = (1-SPLIT_PROP))
g <- sample(cut(
seq(nrow(mnist)),
nrow(mnist)*cumsum(c(0,spec)),
labels = names(spec)
))
res <- split(mnist, g)
print(nrow(res$train))
print(nrow(res$val))
print(nrow(res$test))
train_loader <- dataloader(
MNISTDataset(res$train),
batch_size = BATCH_SIZE,
shuffle = TRUE)
val_loader <- dataloader(
MNISTDataset(res$val),
batch_size = BATCH_SIZE,
shuffle = TRUE)
test_loader <- dataloader(
MNISTDataset(res$test),
batch_size = BATCH_SIZE,
shuffle = TRUE)
print(length(train_loader))
print(length(val_loader))
print(length(test_loader))
library(dplyr)
library(ggplot2)
mnist %>%
select(label) %>%
mutate(sample = g) %>%
ggplot(aes(x=label, fill=sample)) +
geom_bar() +
facet_wrap(~sample, scales = "free_y") +
theme_minimal()
# viz image
vizTrain <- function(input){
dimmax <- sqrt(ncol(mnist_train[,-1]))
dimn <- ceiling(sqrt(nrow(input)))
par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
for (i in 1:nrow(input)){
m1 <- matrix(input[i,2:ncol(input)], nrow=dimmax, byrow=T)
m1 <- apply(m1, 2, as.numeric)
m1 <- t(apply(m1, 2, rev))
image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
text(2, 20, col="white", cex=1.2, mnist_train[i, 1])
}
}
mnist_train <- mnist %>%
mutate(sample = g) %>%
filter(sample == "train") %>%
select(-sample)
# visualize
vizTrain(input = mnist_train[1:25,])
# ALTERNATIVE 2: Sequential, Keras style
input_size <- 784
hidden_sizes <- c(128, 64)
output_size <- 10
model <- nn_sequential(
# Layer 1
nn_linear(input_size, hidden_sizes[1]),
nn_relu(),
# Layer 2
nn_linear(hidden_sizes[1], hidden_sizes[2]),
nn_relu(),
# Layer 3
nn_linear(hidden_sizes[2], output_size)
)
model
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
evaluate_accuracy <- function(logits, y_true) {
y_pred <- logits$argmax(dim=2)
correct_pred <- (y_pred == y_true)
acc <- correct_pred$sum()$item() / y_true$size()
acc * 100
}
model <- torch_load("model.rt")
test_losses <- c()
total <- 0
test_losses <- c()
total <- 0
correct <- 0
y_pred <- c()
y_true <- c()
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
labels <- b[[2]]$to()
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
total <- total + labels$size(1)
y_pred <- c(y_pred, predicted)
y_true <-  c(y_true, labels)
# add number of correct classifications in this batch to the aggregate
# correct <- correct + (predicted == labels)$sum()$item()
}
y_pred
y_pred %>% as_array()
predicted[1] %>% as_array()
525*16
y_pred[[1]]
y_true[[1]]
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
labels <- b[[2]]$to()
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
total <- total + labels$size(1)
y_pred <- c(y_pred, predicted-1)
y_true <-  c(y_true, labels-1)
# add number of correct classifications in this batch to the aggregate
# correct <- correct + (predicted == labels)$sum()$item()
}
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
labels <- b[[2]]$to()
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
total <- total + labels$size(1)
y_pred <- c(y_pred, predicted)
y_true <-  c(y_true, labels)
# add number of correct classifications in this batch to the aggregate
# correct <- correct + (predicted == labels)$sum()$item()
}
model <- torch_load("model.rt")
# model <- torch_load("model_softmax.rt")
test_losses <- c()
total <- 0
correct <- 0
y_pred <- c()
y_true <- c()
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
labels <- b[[2]]$to()
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
total <- total + labels$size(1)
y_pred <- c(y_pred, predicted)
y_true <-  c(y_true, labels)
# add number of correct classifications in this batch to the aggregate
# correct <- correct + (predicted == labels)$sum()$item()
}
mean(test_losses)
as.data.frame(y_pred)
class(y_pred)
y_pred[[1]]
y_pred
as.vector(y_pred)
x <- as.vector(y_pred)
x <- as.matrix(y_pred)
x
x <- unlist(y_pred)
x
x <- as_array(x = y_pred)
model <- torch_load("model.rt")
# model <- torch_load("model_softmax.rt")
test_losses <- c()
total <- 0
y_pred <- torch_tensor()
y_pred <- torch_empty()
y_pred <- torch_empty(c(525, 16))
model <- torch_load("model.rt")
# model <- torch_load("model_softmax.rt")
test_losses <- c()
total <- 0
y_pred <- torch_empty(c(525, 16))
y_true <- torch_empty(c(525, 16))
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
labels <- b[[2]]$to()
loss <- nnf_cross_entropy(output, labels)
test_losses <- c(test_losses, loss$item())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]]
total <- total + labels$size(1)
y_pred <- torch_cat(list(y_pred, predicted))
# c(y_pred, predicted)
y_true <- torch_cat(list(y_true, labels))
# c(y_true, labels)
# add number of correct classifications in this batch to the aggregate
# correct <- correct + (predicted == labels)$sum()$item()
}
mnist_data_unlabeled <- read.csv("mnist/test.csv")
# viz image
vizUnlabled <- function(input){
dimmax <- sqrt(ncol(mnist_data_unlabeled))
dimn <- ceiling(sqrt(nrow(input)))
par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
for (i in 1:nrow(input)){
m1 <- matrix(input[i,1:ncol(input)], nrow=dimmax, byrow=T)
m1 <- apply(m1, 2, as.numeric)
m1 <- t(apply(m1, 2, rev))
image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
}
}
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:25,])
model <- torch_load("model.rt")
y_pred <- c()
y_true <- c()
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
labels <- b[[2]]$to() - 1
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:16){
y_pred <- c(y_pred, predicted[i]$item())
y_true <- c(y_true, labels[i]$item())
}
}
y_pred
library(caret)
library(caret)
confusionMatrix(y_pred, y_true)
confusionMatrix(as.factor(y_pred),as.factor(y_true))
library(yardstick)
conf_mat(y_pred, y_true)
conf_mat(as.factor(y_pred), as.factor(y_true))
conf_mat(factor(y_pred), factor(y_true))
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true)
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 2, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = "Spectral", direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 1, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 3, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 4, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 5, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 6, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 7, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 8, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 9, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 10, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 7, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 4, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 5, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 3, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 6, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 9, direction = "reverse")
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 7, direction = "reverse")
library(yardstick)
data.frame(y_pred = factor(y_pred),
y_true = factor(y_true)) %>%
conf_mat(y_pred, y_true) %>%
autoplot(type = "heatmap") +
scale_fill_distiller(palette = 7, direction = "reverse")
library(caret)
confusionMatrix(as.factor(y_pred),as.factor(y_true))
y_pred_unlabeled <- c()
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred, predicted[i]$item())
}
}
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:25,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:5,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:3,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:10,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:25,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:8,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:10,])
# visualize
vizUnlabled(input = mnist_data_unlabeled[1:12,])
y_pred_unlabeled
y_pred_unlabeled
as.data.frame(y_pred_unlabeled = y_pred_unlabeled)
y_pred_unlabeled <- c()
for (b in enumerate(test_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred, predicted[i]$item())
}
}
as.data.frame(y_pred_unlabeled = y_pred_unlabeled)
y_pred_unlabeled
data.frame(y_pred_unlabeled = y_pred_unlabeled)
data.frame(y_pred_unlabeled = y_pred_unlabeled) %>%
head(12)
mnist_data_unlabeled <- read.csv("mnist/test.csv")
unlabeled_loader <- dataloader(
MNISTDataset(mnist_data_unlabeled),
batch_size = BATCH_SIZE,
shuffle = TRUE)
print(length(unlabeled_loader))
# viz image
vizUnlabled <- function(input){
dimmax <- sqrt(ncol(mnist_data_unlabeled))
dimn <- ceiling(sqrt(nrow(input)))
par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
for (i in 1:nrow(input)){
m1 <- matrix(input[i,1:ncol(input)], nrow=dimmax, byrow=T)
m1 <- apply(m1, 2, as.numeric)
m1 <- t(apply(m1, 2, rev))
image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
}
}
y_pred_unlabeled <- c()
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred, predicted[i]$item())
}
}
# scientific notation
options(scipen = 99)
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.align = "center",
comment = "#>"
)
# install.packages("torch")
library(torch)
MNISTDataset <- dataset(
name = "MNISTDataset",
# init function
initialize = function(df) {
target_col <- 'label'
self$label_exist <- target_col %in% names(df)
if (self$label_exist) {
X <- df[,-which(names(df) == target_col)]
y <- df[,target_col]
# perlu tambahin satu soalnya di R ga bisa akses index 0
self$y <- torch_tensor(as.integer(y+1))
} else {
X <- df
}
self$X <- torch_tensor(as.matrix(X), dtype=torch_float32())
self$X <- self$X / 255
},
.getitem = function(index) {
if (self$label_exist) {
list(X = self$X[index,], y = self$y[index])
} else {
list(X = self$X[index,])
}
},
.length = function() {
self$X$size()[1]
}
)
SPLIT_PROP <- 0.8
BATCH_SIZE <- 16
mnist_data_unlabeled <- read.csv("mnist/test.csv")
unlabeled_loader <- dataloader(
MNISTDataset(mnist_data_unlabeled),
batch_size = BATCH_SIZE,
shuffle = TRUE)
print(length(unlabeled_loader))
# viz image
vizUnlabled <- function(input){
dimmax <- sqrt(ncol(mnist_data_unlabeled))
dimn <- ceiling(sqrt(nrow(input)))
par(mfrow=c(dimn, dimn), mar=c(.1, .1, .1, .1))
for (i in 1:nrow(input)){
m1 <- matrix(input[i,1:ncol(input)], nrow=dimmax, byrow=T)
m1 <- apply(m1, 2, as.numeric)
m1 <- t(apply(m1, 2, rev))
image(1:dimmax, 1:dimmax, m1, col=grey.colors(255), xaxt = 'n', yaxt = 'n')
}
}
model <- torch_load("model.rt")
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred, predicted[i]$item())
}
}
y_pred_unlabeled <- c()
for (b in enumerate(unlabeled_loader)) {
output <- model(b[[1]]$to())
# torch_max returns a list, with position 1 containing the values
# and position 2 containing the respective indices
predicted <- torch_max(output$data(), dim = 2)[[2]] - 1 # supaya label menjadi 0-9
for (i in 1:BATCH_SIZE){
y_pred_unlabeled <- c(y_pred_unlabeled, predicted[i]$item())
}
}
gc()
